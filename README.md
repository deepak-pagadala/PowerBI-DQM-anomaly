# Data Quality Management & Anomaly Detection System
## Power BI | Power Query | DAX

A production-ready Data Quality Management (DQM) system built in Power BI with comprehensive ETL data validation, reconciliation logic, and statistical anomaly detection.

![Architecture](./docs/architecture.png)

---

## ğŸ¯ Key Features

- **Automated Data Quality Checks** â€“ Validates completeness, validity, uniqueness, and referential integrity
- **DQ Scorecard** â€“ Per-table metrics with drill-through to flagged records
- **Order-Payment Reconciliation** â€“ Identifies mismatches between order totals and captured payments
- **Anomaly Detection** â€“ Statistical outlier detection using rolling 14-day sigma rules plus Power BI's built-in anomaly finder
- **Production-Ready** â€“ Configured for scheduled refresh, optional Dataflows/Fabric integration, and Row-Level Security

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ data/                    # Sample CSV files (synthetic data with intentional issues)
â”œâ”€â”€ pbix/                    # Power BI report file (.pbix) and template (.pbit)
â”œâ”€â”€ dax/                     # DAX measures organized by feature area
â”œâ”€â”€ docs/                    # Screenshots and architecture diagrams
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
- Power BI Desktop (latest version)
- Basic understanding of Power Query and DAX

### Getting Started

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/dqm-anomaly-detection.git
   cd dqm-anomaly-detection
   ```

2. **Open the report**
   - Option A: Open `./pbix/DQM_Anomaly.pbix` directly
   - Option B: Use the template `./pbix/DQM_Anomaly.pbit` and point to your `./data/` folder when prompted

3. **Review the ETL logic**
   - Go to **Transform Data** in Power BI Desktop
   - Explore the staging queries and data quality flag logic

4. **Explore the dashboards**
   - **Executive Summary** â€“ High-level KPIs and overall DQ score
   - **DQ Scorecard** â€“ Detailed metrics per table with drill-through
   - **Reconciliation** â€“ Order vs payment mismatches
   - **Anomaly Detection** â€“ Statistical outliers with visualizations
   - **Day Details** â€“ Drill-through page for granular analysis

---

## ğŸ“Š Data Model

### Gold Tables (Cleaned)
- `dim_customers_gold` â€“ Validated customer records
- `dim_products_gold` â€“ Product catalog with schema drift handling
- `fact_orders_gold` â€“ Orders passing all validation rules
- `fact_order_items_gold` â€“ Line items with valid quantities and prices
- `fact_payments_gold` â€“ Payment transactions with capture status
- `fact_deliveries_gold` â€“ Shipping and delivery records
- `fact_order_recon` â€“ Reconciliation between items and payments
- `daily_metrics_enriched` â€“ Aggregated metrics with anomaly flags

### Issue Tables (Audit Trail)
- `dq_customers_issues`
- `dq_products_issues`
- `dq_orders_issues`
- `dq_order_items_issues`
- `dq_payments_issues`
- `dq_deliveries_issues`

---

## âœ… Data Quality Rules

### Completeness
- Required fields must not be null (email, shipping_cost, etc.)

### Domain Validation
- State codes must be in approved list: CA, NY, TX, FL, IL, WA, MA, GA, NC, PA, IN
- Enum fields validated against allowed values

### Uniqueness
- No duplicate `customer_id` or `product_id`

### Referential Integrity
- All `product_id` in order items must exist in products table
- All `customer_id` in orders must exist in customers table

### Temporal Logic
- `order_datetime` â‰¥ `signup_date`
- `delivery_date` â‰¥ `ship_date` â‰¥ `order_date`

### Range Validation
- `quantity` â‰¥ 1
- `unit_price` > 0
- `amount` â‰¥ 0

---

## ğŸ”§ ETL Process

Each source table follows this pattern:

1. **Staging Query** â€“ Applies validation rules and adds flag columns
2. **Issue Reference** â€“ Filters `has_issue = 1` for audit
3. **Gold Reference** â€“ Filters `has_issue = 0` and removes helper columns

### Example Flags (Order Items)

```m
// Quantity validation
qty_invalid = if [quantity] = null or [quantity] < 1 then 1 else 0

// Price validation  
price_invalid = if [unit_price] = null or [unit_price] <= 0 then 1 else 0

// Foreign key validation
fk_product_orphan = if Table.IsEmpty([prod]) then 1 else 0

// Composite flag
has_issue = if qty_invalid = 1 or price_invalid = 1 or fk_product_orphan = 1 then 1 else 0
```

---

## ğŸ“ˆ Key DAX Measures

### DQ Score Pattern
```dax
Customers Valid = COUNTROWS('dim_customers_gold')
Customers Issues = COUNTROWS('dq_customers_issues')
Customers Total = [Customers Valid] + [Customers Issues]
Customers Issue Rate (%) = DIVIDE([Customers Issues], [Customers Total], 0)
Customers DQ Score = 1 - [Customers Issue Rate (%)]
```

### Reconciliation
```dax
Orders with Mismatch = 
CALCULATE(
    DISTINCTCOUNT('fact_order_recon'[order_id]),
    'fact_order_recon'[status_mismatch_flag] = 1
)
```

### Anomaly Detection
```dax
Anomaly Count (Orders) = SUM('daily_metrics_enriched'[orders_anomaly_flag])

Anomaly Rate (Orders) - Last 30d = 
VAR maxDate = MAX('daily_metrics_enriched'[date])
VAR days = CALCULATE(
    DISTINCTCOUNT('daily_metrics_enriched'[date]), 
    'daily_metrics_enriched'[date] > maxDate - 30
)
RETURN DIVIDE([Anomaly Count (Orders) - Last 30d], days, 0)
```

Full DAX library available in `/dax/`.

---

## ğŸŒ Deployment

### Power BI Service
1. Publish the report to your workspace
2. Configure data source credentials
3. Set up scheduled refresh (daily/hourly recommended)

### Data Source Options
- **Cloud**: Store CSVs on OneDrive/SharePoint for automatic refresh
- **On-Premises**: Configure On-premises Data Gateway for local files

### Optional Enhancements
- **Dataflows/Fabric**: Migrate Power Query logic to Dataflows Gen2 for centralized ETL (see `/deployment/dataflows-notes.md`)
- **Row-Level Security**: Implement filters by state or channel (see `/deployment/rls-policy.md`)

---

## ğŸ“¸ Screenshots

### Data Models
![Data Models](./docs/datamodels.jpg)

### DQ Scorecard
![DQ Scorecard](./docs/scorecard.jpg)

### Anomaly Detection
![Anomaly Detection](./docs/anomalies.jpg)


---

## ğŸ—ºï¸ Roadmap

- [ ] Implement data alerts for DQ Score < 95%
- [ ] Add monthly trend analysis for DQ issues
- [ ] Integrate Prophet/ARIMA for advanced forecasting (Fabric)
- [ ] Create unit tests for Power Query M scripts
- [ ] Add CI/CD pipeline for automated deployment

---

## ğŸ“ Testing

The `/tests/` folder contains:
- **dq-rules-checklist.md** â€“ Complete validation rule documentation
- **validation-queries.md** â€“ Sample queries to verify data quality

Run these checks after modifying ETL logic to ensure integrity.

---

## ğŸ”’ License

MIT License - see [LICENSE](LICENSE) file for details

---


## âš ï¸ Data Notice

This project uses synthetic data with intentional quality issues for demonstration purposes. Replace files in `/data/` with your actual data sources. The ETL and DQ logic is designed to handle real-world scenarios.

---

**Built with** âš¡ Power BI | ğŸ”„ Power Query | ğŸ“Š DAX
